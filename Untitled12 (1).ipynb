{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f2f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d405ad84",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bfac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.datasets import get_data\n",
    "data_df = get_data('diamonds')\n",
    "#check the shape of data\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf254923",
   "metadata": {},
   "source": [
    "In order to demonstrate the use of the predict_model function on test data, a sample of 600 records has been divided from the original dataset to be used for predictions. This should not be confused with a train/test split as this particular split is performed to simulate a real-life scenario. Another way to think about this is that these 600 records are not available at the time when this machine learning experiment was performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e19a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = dataset.sample(frac=0.9, random_state=786)\n",
    "data_unseen = dataset.drop(data_df.index)\n",
    "\n",
    "data_df.reset_index(drop=True, inplace=True)\n",
    "data_unseen.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print('Data for Modeling: ' + str(data_df.shape))\n",
    "print('Unseen Data For Predictions: ' + str(data_unseen.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b335b60f",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "do some quick visualization to assess the relationship of independent features (weight, cut, color, clarity, etc.) with the target variable i.e. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5297ed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scatter carat_weight and Price\n",
    "import plotly.express as px\n",
    "fig = px.scatter(x=data_df['Carat Weight'], y=data_df['Price'], facet_col = data_df['Cut'], opacity = 0.25, template = 'plotly_dark', trendline='ols', trendline_color_override = 'red', title = 'SARAH GETS A DIAMOND - A CASE STUDY')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674e0da0",
   "metadata": {},
   "source": [
    "check the distribution of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2920c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram\n",
    "fig = px.histogram(data_df, x=[\"Price\"], template = 'plotly_dark', title = 'Histogram of Price')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac3c507",
   "metadata": {},
   "source": [
    "that the distribution of Price is right-skewed, we can quickly check to see if log transformation can make Price approximately normal to give fighting chance to algorithms that assume normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae583321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of data\n",
    "data_copy = data_df.copy()\n",
    "\n",
    "# create a new feature Log_Price\n",
    "data_copy['Log_Price'] = np.log(data_df['Price'])\n",
    "\n",
    "# plot histogram\n",
    "fig = px.histogram(data_copy, x=[\"Log_Price\"], title = 'Histgram of Log Price', template = 'plotly_dark')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c476d",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1c10a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c18c4d3",
   "metadata": {},
   "source": [
    "# Setting up the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e550619",
   "metadata": {},
   "source": [
    "we need to set up the environment using the setup function. This function initializes the experiment in PyCaret, handles various preprocessing tasks, and allows users to customize the behavior of the machine learning pipeline based on the parameters passed in the function. Here are some important parameters of the setup() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f45e4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.regression import *\n",
    "s = setup(data, target = 'Price', transform_target = True, log_experiment = True, experiment_name = 'diamonds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eaefc7",
   "metadata": {},
   "source": [
    "Now our environment is fully functional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba412fa",
   "metadata": {},
   "source": [
    "# Comparing all Regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4c65f5",
   "metadata": {},
   "source": [
    "With just a single line of code, you can run your training set on all of the available models in PyCaret. You can view the models available by typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b016f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the models that are available are \n",
    "models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd17401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the best one \n",
    "best = compare_models(exclude = ['ransac'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0c765",
   "metadata": {},
   "source": [
    "the compare_models return the best performing model based on default sort order but can be used to return a list of top N models by using n_select parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4accde8",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849d0862",
   "metadata": {},
   "source": [
    "create_model is the most granular function in PyCaret and is often the foundation behind most of the PyCaret functionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb0c1f6",
   "metadata": {},
   "source": [
    "    AdaBoost Regressor (‘ada’)\n",
    "    Light Gradient Boosting Machine (‘lightgbm’)\n",
    "    Decision Tree (‘dt’)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0b9db5",
   "metadata": {},
   "source": [
    "# AdaBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ae1f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = create_model('ada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705e9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b1ab77",
   "metadata": {},
   "source": [
    "# Light Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326fa1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightgbm = create_model('lightgbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9b1cdf",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c4418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = create_model('dt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f5ff68",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e473800",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_best_clf = evaluate_model(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d6d258",
   "metadata": {},
   "source": [
    "Here are a few more charts on the performance of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908b8600",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(best_reg, plot = 'learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbefcdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(best_reg, plot = 'error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f7a82b",
   "metadata": {},
   "source": [
    "Here is an overview of possible graphics in PyCaret: Examples by module - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc79e083",
   "metadata": {},
   "source": [
    "# Saving image files\n",
    "If you want to save the output graphics in PyCaret, you have to set the safe parameter to True. The syntax would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbe7632",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(best_reg, plot = 'error',save = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0973f12",
   "metadata": {},
   "source": [
    "# Model Optimization\n",
    "will try to improve the performance of our created algorithm with different methods. At the end of the chapter I will create an overview of the performance values. On their basis I will select afterwards the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feebf6f6",
   "metadata": {},
   "source": [
    "# Tune the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacf77e6",
   "metadata": {},
   "source": [
    "AdaBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48119c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_ada = tune_model(ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff922b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tuned_ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01914172",
   "metadata": {},
   "source": [
    "Light Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0e0291",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = {'num_leaves': np.arange(10,200,10),\n",
    "                        'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "                        'learning_rate': np.arange(0.1,1,0.1)\n",
    "                        }tuned_lightgbm = tune_model(lightgbm, custom_grid = lgbm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f39afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tuned_lightgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f8a3f0",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faded2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_dt = tune_model(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6ec984",
   "metadata": {},
   "source": [
    "# Plot a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c4dd02",
   "metadata": {},
   "source": [
    "Before finalizing the model, the plot_model() function can be used to evaluate the performance of the model across different aspects such as Residual Plots, Prediction Error, Feature Importance, etc.\n",
    "\n",
    "There are over 10 plots available under plot_model(), which you can view by typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c982509",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6731718",
   "metadata": {},
   "source": [
    "# Residual Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02767681",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(tuned_lightgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638da4fd",
   "metadata": {},
   "source": [
    "# Prediction Error Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39db9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(tuned_lightgbm, plot = 'error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a677171",
   "metadata": {},
   "source": [
    "# Feature Importance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c9ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(tuned_lightgbm, plot='feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046e016b",
   "metadata": {},
   "source": [
    "Another nice way of analyzing the model is to use the evaluate_model() function which creates an Interactive dashboard with all the available plots to choose from. The user can easily select an option and view the plot of their choice. The plot_model() function is used internally. For example, here I’ve chosen the ‘Cooks Distance’ plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201ea98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(tuned_lightgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a51763",
   "metadata": {},
   "source": [
    "# Prediction on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214221f8",
   "metadata": {},
   "source": [
    "perform one final check by predicting the test/hold-out set and reviewing the evaluation metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c1d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_model(tuned_lightgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e692742a",
   "metadata": {},
   "source": [
    "# Finalizing model for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e5adb",
   "metadata": {},
   "source": [
    "The finalize_model function fits the model onto the complete dataset including the test/hold-out sample (30% in this case). The purpose of this function is to train the model on the complete dataset before it is deployed in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c016ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lightgbm = finalize_model(tuned_lightgbm)print(final_lightgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5680741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_model(final_lightgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd826f6",
   "metadata": {},
   "source": [
    "# Predicting on the Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490653e9",
   "metadata": {},
   "source": [
    "What the below code does is adds the predicted value to a new column called “Label” at the end of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b077dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_predictions = predict_model(final_lightgbm, data=data_unseen)\n",
    "unseen_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f999e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the metrics on this since you have an actual target column Price available.\n",
    "from pycaret.utils import check_metric\n",
    "check_metric(unseen_predictions.Price, unseen_predictions.Label, 'R2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c587ac",
   "metadata": {},
   "source": [
    "# Saving the model\n",
    "We have now finished the experiment, and have used the stored model called final_gbr to predict the unseen data. But what happens when we have new data to predict? Do we have to start from scratch and create a model again? Well, the answer is obviously No. PyCaret’s inbuilt save_model() allows us to save this already trained model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f0b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(final_lightgbm,'Final LightGBM Model 25Nov2020')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77df3712",
   "metadata": {},
   "source": [
    "you can simply use it to predict any new data using the same predict_model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c7de71",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_prediction = predict_model(saved_final_lightgbm, data=data_unseen)new_prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8df55dd",
   "metadata": {},
   "source": [
    "we have applied the loaded model to predict the same data_unseen that we used in above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7335b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.utils import check_metric\n",
    "check_metric(new_prediction.Price, new_prediction.Label, 'R2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b424db9",
   "metadata": {},
   "source": [
    "the results of unseen_predictions and new_prediction are identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722cf60b",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "covered the entire machine learning pipeline starting from loading the data, pre-processing the data, training model, hyperparameter tuning, and at last the prediction and saving of the trained model. All this is done in less than 10 commands which are intuitive and easy to remember. Recreating this whole process without the use of PyCaret would’ve taken 100s of lines of codes using the normal libraries, but these are only the basics of the pycaret.regression module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bb2e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
